# -*- coding: utf-8 -*-
"""trader_sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cNwzabsB-XImaPHxZpRV0GmIpFlROd_0

Enhanced Trader Behavior vs Market Sentiment Analysis

Place CSVs in same folder:
 - bitcoin_market_sentiment.csv  (cols: timestamp, value, classification, date)
 - historical_trader_data.csv    (cols: Account, Coin, Execution Price, Size Tokens, Size USD,Side, Timestamp IST, Start Position, Direction, Closed PnL, Fee, Leverage, ...)

Outputs:
 - ./outputs/ (CSVs & text summaries)
 - ./outputs/plots/ (PNG figures)

Dependencies:
 pandas, numpy, matplotlib, seaborn, scipy, scikit-learn
"""

import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# ---------------- Setup ----------------
OUTDIR = Path('./outputs')
PLOTS = OUTDIR / 'plots'
OUTDIR.mkdir(parents=True, exist_ok=True)
PLOTS.mkdir(parents=True, exist_ok=True)

sns.set_style('whitegrid')

# ---------------- Load ----------------
BTC_FILE = '/content/fear_greed_index.csv'
TRADES_FILE = '/content/historical_data.csv'

print("Loading files...")
btc = pd.read_csv(BTC_FILE)
trades = pd.read_csv(TRADES_FILE)

# ---------------- Preprocess BTC sentiment ----------------
# Normalize classification and map Extreme Fear -> Fear if desired
if 'classification' in btc.columns:
    btc['classification'] = btc['classification'].astype(str).str.strip().str.title()
    btc['classification'] = btc['classification'].replace({'Extreme Fear':'Fear', 'Extreme Greed':'Greed'})
else:
    raise ValueError('Sentiment file must contain "classification" column')

# Ensure date column
if 'date' in btc.columns:
    btc['date'] = pd.to_datetime(btc['date']).dt.date
elif 'timestamp' in btc.columns:
    # if unix timestamp in seconds
    try:
        btc['date'] = pd.to_datetime(btc['timestamp'], unit='s').dt.date
    except Exception:
        btc['date'] = pd.to_datetime(btc['timestamp']).dt.date
else:
    raise ValueError('Sentiment file needs a date or timestamp column')

# numeric sentiment value (if column exists)
if 'value' in btc.columns:
    btc['sentiment_value'] = pd.to_numeric(btc['value'], errors='coerce')
else:
    btc['sentiment_value'] = np.nan

btc_daily = btc[['date','sentiment_value','classification']].rename(columns={'classification':'sentiment_label'})

# ---------------- Preprocess trades ----------------
# Normalize column names for convenience
trades.columns = [c.strip() for c in trades.columns]

# Attempt to locate time, price, size, pnl, leverage columns
time_col = None
for c in trades.columns:
    if 'time' in c.lower():
        time_col = c
        break
if time_col is None:
    raise ValueError("Cannot find time column in trades CSV")

# Parse Timestamp IST like "02-12-2024 22:50" (d-m-Y H:M)
trades['time'] = pd.to_datetime(trades[time_col], errors='coerce', dayfirst=True)
trades['date'] = trades['time'].dt.date

# Execution price column
price_cols = [c for c in trades.columns if 'execution' in c.lower() or 'price'==c.lower()]
if price_cols:
    trades['execution_price'] = pd.to_numeric(trades[price_cols[0]], errors='coerce')
else:
    trades['execution_price'] = np.nan

# Size (use Size USD if available as notional, otherwise numeric size * price)
if 'Size USD' in trades.columns:
    trades['size_usd'] = pd.to_numeric(trades['Size USD'], errors='coerce')
else:
    # try other columns
    size_cols = [c for c in trades.columns if 'size' in c.lower() and 'usd' not in c.lower()]
    if size_cols and not trades['execution_price'].isna().all():
        trades['size_tokens'] = pd.to_numeric(trades[size_cols[0]], errors='coerce')
        trades['size_usd'] = trades['size_tokens'] * trades['execution_price']
    else:
        trades['size_usd'] = np.nan

# Side normalization
if 'Side' in trades.columns:
    trades['side'] = trades['Side'].astype(str).str.lower().str.strip()
else:
    trades['side'] = trades['side'].astype(str).str.lower().str.strip()

trades['side'] = trades['side'].replace({'buy':'buy','sell':'sell','long':'buy','short':'sell'})

# Closed PnL
pnl_cols = [c for c in trades.columns if 'closed' in c.lower() and 'pnl' in c.lower()]
if pnl_cols:
    trades['closed_pnl'] = pd.to_numeric(trades[pnl_cols[0]], errors='coerce').fillna(0.0)
else:
    trades['closed_pnl'] = 0.0

# Leverage
lev_col = None
for c in trades.columns:
    if 'leverage' == c.lower() or 'leverage' in c.lower():
        lev_col = c
        break
if lev_col:
    trades['leverage'] = pd.to_numeric(trades[lev_col], errors='coerce')
else:
    trades['leverage'] = np.nan  # will try proxy later

# Notional
trades['notional'] = trades['size_usd']

# Signed notional (buy positive, sell negative)
trades['signed_notional'] = trades['notional'] * np.where(trades['side']=='buy', 1, -1)

# Flag winner
trades['winner'] = trades['closed_pnl'] > 0

# ---------------- Daily aggregations with risk metrics ----------------
print("Computing daily aggregates...")
def daily_agg(df):
    g = df.groupby('date')
    agg = g.agg(
        trades_count = ('notional','count'),
        total_notional = ('notional','sum'),
        avg_notional = ('notional','mean'),
        median_notional = ('notional','median'),
        realized_pnl = ('closed_pnl','sum'),
        avg_pnl = ('closed_pnl','mean'),
        pnl_std = ('closed_pnl','std'),
        pnl_5pct = ( 'closed_pnl', lambda x: np.nan if len(x)==0 else np.nanpercentile(x,5) ),
        win_rate = ('winner','mean'),
        avg_leverage = ('leverage','mean'),
        median_leverage = ('leverage','median'),
        buy_ratio = ('side', lambda s: (s=='buy').mean() )
    ).reset_index()
    return agg

daily = daily_agg(trades)

# If leverage entirely missing, build a simple proxy:
if daily['avg_leverage'].isna().all():
    print("Leverage column missing — building a proxy (notional / median account notional)")
    # compute per-account median notional
    if 'Account' in trades.columns:
        acct_med = trades.groupby('Account')['notional'].median().replace(0,np.nan)
        # map per-trade account median and compute proxy
        trades['acct_med_notional'] = trades['Account'].map(acct_med)
        trades['leverage_proxy'] = trades['notional'] / trades['acct_med_notional']
        trades['leverage_proxy'].replace([np.inf, -np.inf], np.nan, inplace=True)
        daily_proxy = trades.groupby('date')['leverage_proxy'].mean().reset_index().rename(columns={'leverage_proxy':'avg_leverage_proxy'})
        daily = daily.merge(daily_proxy, on='date', how='left')
        daily['avg_leverage'] = daily['avg_leverage_proxy']
        daily.drop(columns=['avg_leverage_proxy'], inplace=True)
    else:
        daily['avg_leverage'] = np.nan

# Merge with btc_sentiment
merged = pd.merge(daily, btc_daily, left_on='date', right_on='date', how='left')

# Save merged
merged.to_csv(OUTDIR / 'merged_daily_with_sentiment_enhanced.csv', index=False)

# ---------------- Rolling features ----------------
print("Computing rolling features (7/14/30 days)...")
merged = merged.sort_values('date')
for w in [7, 14, 30]:
    merged[f'realized_pnl_roll_mean_{w}'] = merged['realized_pnl'].rolling(w, min_periods=1).mean()
    merged[f'realized_pnl_roll_std_{w}'] = merged['realized_pnl'].rolling(w, min_periods=1).std().fillna(0)
    merged[f'total_notional_roll_mean_{w}'] = merged['total_notional'].rolling(w, min_periods=1).mean()
    merged[f'sentiment_roll_mean_{w}'] = merged['sentiment_value'].rolling(w, min_periods=1).mean()

merged.to_csv(OUTDIR / 'merged_with_rolling_features.csv', index=False)

# ---------------- Cross-correlation (lead/lag) ----------------
print("Computing lead/lag cross-correlations (sentiment vs realized_pnl and total_notional)...")
def cross_corr_series(a, b, max_lag=14):
    # a and b are pandas series aligned by index (time series)
    res = []
    for lag in range(-max_lag, max_lag+1):
        if lag < 0:
            x = a.shift(-lag)  # a leads
            y = b
        else:
            x = a
            y = b.shift(lag)  # b leads
        valid = x.notna() & y.notna()
        if valid.sum() < 5:
            res.append(np.nan)
        else:
            res.append(x[valid].corr(y[valid]))
    lags = list(range(-max_lag, max_lag+1))
    return pd.DataFrame({'lag':lags, 'corr':res})

sent_series = merged['sentiment_value']
pnl_series = merged['realized_pnl']
notional_series = merged['total_notional']

cc_pnl = cross_corr_series(sent_series, pnl_series, max_lag=14)
cc_not = cross_corr_series(sent_series, notional_series, max_lag=14)

cc_pnl.to_csv(OUTDIR / 'crosscorr_sentiment_vs_pnl.csv', index=False)
cc_not.to_csv(OUTDIR / 'crosscorr_sentiment_vs_notional.csv', index=False)

# Plot cross-corrs
plt.figure(figsize=(8,4))
plt.plot(cc_pnl['lag'], cc_pnl['corr'], marker='o')
plt.axvline(0, color='k', linestyle='--')
plt.title('Cross-correlation (sentiment vs realized_pnl) — positive lag: pnl leads')
plt.xlabel('Lag (days)')
plt.ylabel('Correlation')
plt.tight_layout()
plt.savefig(PLOTS / 'crosscorr_sentiment_pnl.png')
plt.close()

plt.figure(figsize=(8,4))
plt.plot(cc_not['lag'], cc_not['corr'], marker='o')
plt.axvline(0, color='k', linestyle='--')
plt.title('Cross-correlation (sentiment vs total_notional)')
plt.xlabel('Lag (days)')
plt.ylabel('Correlation')
plt.tight_layout()
plt.savefig(PLOTS / 'crosscorr_sentiment_notional.png')
plt.close()

# ---------------- Statistical tests ----------------
print("Running robust statistical tests (Mann-Whitney U + Cliff's delta)...")
merged['sentiment_label'] = merged['sentiment_label'].fillna('Unknown')
greed = merged[merged['sentiment_label']=='Greed']
fear = merged[merged['sentiment_label']=='Fear']

def cliffs_delta(a, b):
    # compute small-sample estimator for Cliff's delta
    a = np.array(a.dropna())
    b = np.array(b.dropna())
    n, m = len(a), len(b)
    if n==0 or m==0:
        return np.nan
    greater = 0
    lesser = 0
    for ai in a:
        greater += (ai > b).sum()
        lesser += (ai < b).sum()
    delta = (greater - lesser) / (n*m)
    return delta

with open(OUTDIR / 'stat_tests.txt','w') as f:
    for var in ['realized_pnl','total_notional','avg_leverage','pnl_std','win_rate']:
        a = greed[var].dropna() if var in greed.columns else pd.Series(dtype=float)
        b = fear[var].dropna() if var in fear.columns else pd.Series(dtype=float)
        if len(a) < 3 or len(b) < 3:
            f.write(f'Not enough data for {var}\n')
            continue
        stat, p = stats.mannwhitneyu(a, b, alternative='two-sided')
        delta = cliffs_delta(a,b)
        f.write(f'Variable: {var}\n Mann-Whitney U stat={stat:.4f}, p={p:.4f}\n Cliff\'s delta={delta:.4f}\n\n')

# ---------------- Visualizations ----------------
print("Saving visualizations...")
# Time series: realized pnl and sentiment
plt.figure(figsize=(12,5))
ax = plt.gca()
ax.plot(merged['date'], merged['realized_pnl_roll_mean_7'], label='7d mean realized_pnl')
ax2 = ax.twinx()
ax2.plot(merged['date'], merged['sentiment_roll_mean_7'], color='orange', label='7d mean sentiment')
ax.set_xlabel('Date'); ax.set_ylabel('Realized PnL (7d mean)')
ax2.set_ylabel('Sentiment (7d mean)')
ax.legend(loc='upper left'); ax2.legend(loc='upper right')
plt.title('Realized PnL vs Sentiment (7-day rolling)')
plt.tight_layout()
plt.savefig(PLOTS / 'pnl_vs_sentiment_rolling.png')
plt.close()

# Boxplots: pnl by sentiment
plt.figure(figsize=(10,5))
sns.boxplot(x='sentiment_label', y='realized_pnl', data=merged)
plt.title('Distribution of Daily Realized PnL by Sentiment')
plt.tight_layout()
plt.savefig(PLOTS / 'pnl_by_sentiment_box.png')
plt.close()

# Leverage distribution per sentiment (trade-level)
if 'leverage' in trades.columns or 'leverage' in trades:
    trades_sent = trades.merge(btc_daily, on='date', how='left')
    if 'leverage' in trades_sent.columns and trades_sent['leverage'].notna().sum() > 5:
        plt.figure(figsize=(10,5))
        sns.boxplot(x='sentiment_label', y='leverage', data=trades_sent)
        plt.title('Trade-level Leverage by Sentiment')
        plt.tight_layout()
        plt.savefig(PLOTS / 'leverage_by_sentiment.png')
        plt.close()

# Correlation heatmap (numerical features)
plt.figure(figsize=(8,6))
numcols = merged.select_dtypes(include=[np.number]).columns.tolist()
sns.heatmap(merged[numcols].corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.tight_layout()
plt.savefig(PLOTS / 'correlation_matrix.png')
plt.close()

# ---------------- Simple predictive modeling (classification) ----------------
print("Training classification model to predict pnl_positive using enhanced features...")
merged['pnl_positive'] = merged['realized_pnl'] > 0
# create feature set
features = [
    'sentiment_value','total_notional','avg_notional','pnl_std','win_rate',
    'realized_pnl_roll_mean_7','realized_pnl_roll_std_7','total_notional_roll_mean_7'
]
for f in features:
    if f not in merged.columns:
        merged[f] = 0.0

X = merged[features].fillna(0)
y = merged['pnl_positive'].astype(int).fillna(0)

# require at least some variation
if len(merged) >= 30 and y.nunique()>1:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
    clf = RandomForestClassifier(n_estimators=200, random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_prob = clf.predict_proba(X_test)[:,1]
    with open(OUTDIR / 'model_classification_report.txt','w') as f:
        f.write("RandomForest classification report (pnl_positive):\n")
        f.write(classification_report(y_test, y_pred))
        try:
            f.write(f"\nROC AUC: {roc_auc_score(y_test, y_prob):.4f}\n")
        except Exception:
            pass
    # feature importances
    fi = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)
    fi.to_csv(OUTDIR / 'feature_importances_classification.csv')
    plt.figure(figsize=(6,4))
    sns.barplot(x=fi.values, y=fi.index)
    plt.title('Feature importances (classification)')
    plt.tight_layout()
    plt.savefig(PLOTS / 'feature_importances_classification.png')
    plt.close()
else:
    print("Not enough data/variation for classification modeling")

# ---------------- Simple predictive modeling (regression next-day pnl) ----------------
print("Training regression model to predict next-day realized_pnl sign/size...")
# prepare next-day target
merged = merged.sort_values('date')
merged['realized_pnl_next'] = merged['realized_pnl'].shift(-1)
reg_features = features
df_reg = merged.dropna(subset=['realized_pnl_next'])

if len(df_reg) >= 30:
    Xr = df_reg[reg_features].fillna(0)
    yr = df_reg['realized_pnl_next']
    Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.25, random_state=42)
    reg = RandomForestRegressor(n_estimators=200, random_state=42)
    reg.fit(Xr_train, yr_train)
    y_pred_reg = reg.predict(Xr_test)
    mae = mean_absolute_error(yr_test, y_pred_reg)
    with open(OUTDIR / 'regression_performance.txt','w') as f:
        f.write(f"RandomForest regression MAE (next-day realized_pnl): {mae:.4f}\n")
    # save importances
    fi_reg = pd.Series(reg.feature_importances_, index=reg_features).sort_values(ascending=False)
    fi_reg.to_csv(OUTDIR / 'feature_importances_regression.csv')
    plt.figure(figsize=(6,4))
    sns.barplot(x=fi_reg.values, y=fi_reg.index)
    plt.title('Feature importances (regression)')
    plt.tight_layout()
    plt.savefig(PLOTS / 'feature_importances_regression.png')
    plt.close()
else:
    print("Not enough data for regression modeling")

print("Enhanced analysis complete. Check the 'outputs' folder for CSVs, tests, and plots.")